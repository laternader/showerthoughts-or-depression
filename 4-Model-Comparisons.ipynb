{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content Disclaimer**: This dataset contains real title posts scraped from the r/depression subreddit, and some of the titles contain language that is not safe for work, crude, or offensive. The full dataset is available as `depression.csv`, `preprocessed_thoughts.csv`, `thoughts.csv`, and `token_df.csv`. Unfortunately, I did not provide a sanitized version of my dataset because the words contained were important for the analysis and understanding of the model. Please note that the model, the dataset, and the techniques used are not perfect. If you have any concerns about working with this dataset, looking at my analysis, or the topic in general, you can skip my content overall or click [here](http://iamsorry.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "*If links don't work then it this at least shows the order of the notebook*\n",
    "\n",
    "- [Loading Pickles](https://git.generalassemb.ly/laternader/project_3/blob/master/deliverables/4-Model-Comparisons.ipynb#Load-the-Pickled-Models)\n",
    "- [Scores](https://git.generalassemb.ly/laternader/project_3/blob/master/deliverables/4-Model-Comparisons.ipynb#Let's-look-at-the-scores-from-each)\n",
    "- [Parameters](https://git.generalassemb.ly/laternader/project_3/blob/master/deliverables/4-Model-Comparisons.ipynb#Let's-look-at-the-parameters)\n",
    "- [Train/Test Scores](https://git.generalassemb.ly/laternader/project_3/blob/master/deliverables/4-Model-Comparisons.ipynb#The-train-and-test-scores-for-all-the-models)\n",
    "- [Confusion Matrices](https://git.generalassemb.ly/laternader/project_3/blob/master/deliverables/4-Model-Comparisons.ipynb#Confusion-Matrices)\n",
    "- Post EDA Work\n",
    "    - [Feature Importance](https://git.generalassemb.ly/laternader/project_3/blob/master/deliverables/4-Model-Comparisons.ipynb#Feature-Importance)\n",
    "    - [Good Indicators](https://git.generalassemb.ly/laternader/project_3/blob/master/deliverables/4-Model-Comparisons.ipynb#Which-words-are-good-indicators-outside-of-the-stopwords?)\n",
    "    - [Sentiment Analysis](https://git.generalassemb.ly/laternader/project_3/blob/master/deliverables/4-Model-Comparisons.ipynb#How-sad-are-they?)\n",
    "    - [F1 Scores](https://git.generalassemb.ly/laternader/project_3/blob/master/deliverables/4-Model-Comparisons.ipynb#F1-Scores)\n",
    "- [Conclusion](https://git.generalassemb.ly/laternader/project_3/blob/master/deliverables/4-Model-Comparisons.ipynb#Final-Conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Pickled Models\n",
    "\n",
    "---\n",
    "To keep things organized, I have three models imported. In these models, this is what they stand for.\n",
    "\n",
    "+ `best` - I considered this the best model because the difference between the train and test scores was less than 2 and in the low-mid 90s in accuracy,\n",
    "+ `forest` - The best forest model I could produce where the test score wasn't as far as it was from the train score and where the it was optimized for sensitivity.\n",
    "+ `another` - This model had a reasonable and better train and test score than all of my other models. I didn't consider it the best due to the gap between the train and test score was larger than what I considered the best. I think there is some over fit as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = pickle.load(open('./assets/best_model.pkl','rb'))\n",
    "forest = pickle.load(open('./assets/forest_model.pkl','rb'))\n",
    "another = pickle.load(open('./assets/another_model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at the scores from each model\n",
    "\n",
    "---\n",
    "Looking at the best score, I confirmed my decision on why I selected the specific model as my \"best\". The mean of the cross val scores. These were all pretty much on 5 folds, so the fact that the numbers are the same shows there is some high variance in the `another` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9138805970149255"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9120149253731343"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9193283582089551"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at the parameters\n",
    "\n",
    "---\n",
    "One thing to notice is that what I considered my best model was using a LogisticRegression model. What I considered second best was also a LogisticRegression model. However the parameters differed and the data they took in differed. For the `best` model, I used data that came from me \"PorterStemming\" the post titles and then feeding them through the TfidVectorizer transformer while `another` used the original title post and CountVectorized the titles. This also led to a difference in the amount of max features (the top max_features ordered by term frequency across the corpus), regularization strength and minimum data frequency (the word has to appear a minimum).\n",
    "\n",
    "In my `forest` model, it used almost all defaults except where max_features was set to 'log2' in terms of size and the n_estimators was high at 150. I am sure that if given more time (*and sanity*) I could go back and increase it and maybe the overfitting could have been reduced.\n",
    "\n",
    "The one common thing that all my models shared was not using the stop words in any of the best models. I found that peculiar because I would assume that those words would be not as important in determing if one post was a part of one subreddit or not. However looking at the n_gram_range, I don't think I should be surprised that possibly pairs or triplets might have played a larger role in classifying a post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__C': 1,\n",
       " 'model__penalty': 'l2',\n",
       " 'trans__max_df': 0.85,\n",
       " 'trans__max_features': 3000,\n",
       " 'trans__min_df': 5,\n",
       " 'trans__ngram_range': (1, 3),\n",
       " 'trans__stop_words': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('trans',\n",
       "                 TfidfVectorizer(max_df=0.85, max_features=3000, min_df=5,\n",
       "                                 ngram_range=(1, 3))),\n",
       "                ('model', LogisticRegression(C=1))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__max_depth': None,\n",
       " 'model__max_features': 'log2',\n",
       " 'model__min_samples_leaf': 1,\n",
       " 'model__n_estimators': 150,\n",
       " 'trans__ngram_range': (1, 1),\n",
       " 'trans__stop_words': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('trans', TfidfVectorizer()),\n",
       "                ('model',\n",
       "                 RandomForestClassifier(max_features='log2',\n",
       "                                        n_estimators=150))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__C': 0.95,\n",
       " 'model__penalty': 'l2',\n",
       " 'trans__max_df': 0.85,\n",
       " 'trans__max_features': 6000,\n",
       " 'trans__min_df': 3,\n",
       " 'trans__ngram_range': (1, 3),\n",
       " 'trans__stop_words': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('trans',\n",
       "                 CountVectorizer(max_df=0.85, max_features=6000, min_df=3,\n",
       "                                 ngram_range=(1, 3))),\n",
       "                ('model', LogisticRegression(C=0.95))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The train and test scores for all the models\n",
    "\n",
    "---\n",
    "As I have state previously, the `best` model was chose as the best one because of the difference between train and test scores. Granted since the train scores tend to always be better than test scores, I wanted a model that minimized that as much as possible. So the `best` model fit the bill. \n",
    "\n",
    "And with the `best_score_` being the consistent through all three of these models, I have justification that accounting for overfit is a good way to select my model. This was also why I considered the `forest` model to be worse because although it was a ~.99 accuracy, that didn't account for the fact that there was clear overfitting.\n",
    "\n",
    "In `another`, the train and test scores were very high, very accurate (*for what it's worth*). However, there was still a considerable gap between the two which showed that the model couldn't be generalized. If you look at the upcoming confusion matrices, it becomes apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_train = pickle.load(open('./assets/best_train_score.pkl','rb'))\n",
    "best_test = pickle.load(open('./assets/best_test_score.pkl','rb'))\n",
    "forest_train = pickle.load(open('./assets/forest_train_score.pkl','rb'))\n",
    "forest_test = pickle.load(open('./assets/forest_test_score.pkl','rb'))\n",
    "another_train = pickle.load(open('./assets/another_train_score.pkl','rb'))\n",
    "another_test = pickle.load(open('./assets/another_test_score.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Train Score: 0.9373880597014925\n",
      "Best Test Score: 0.9216666666666666\n",
      "\n",
      "Forest Train Score: 0.9982089552238806\n",
      "Forest Test Score: 0.9168181818181819\n",
      "\n",
      "Another Train Score: 0.9658955223880596\n",
      "Another Test Score: 0.9316666666666666\n"
     ]
    }
   ],
   "source": [
    "print('Best Train Score:', best_train)\n",
    "print('Best Test Score:', best_test)\n",
    "print()\n",
    "print('Forest Train Score:', forest_train)\n",
    "print('Forest Test Score:', forest_test)\n",
    "print()\n",
    "print('Another Train Score:', another_train)\n",
    "print('Another Test Score:', another_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices\n",
    "\n",
    "---\n",
    "One of the main reasons for `best` model to be considered my best model was also due to the fact that the specificity and sensitivity were consistent without optimization. I felt confident that this model could be used in other subreddits, but I will not delve into that too deeply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pickle.load(open('./assets/best_confusionmatrix.pkl','rb'))\n",
    "plt.title('\"Best\" model Confusion Matrix')\n",
    "\n",
    "# plt.savefig('imgs/bestconf.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Specificity: 0.9232643118148599 Best Sensitivity: 0.9200844390832328\n"
     ]
    }
   ],
   "source": [
    "# Calculate specificity and sensitivity by hand since I didn't know how to pickle them properly\n",
    "spec1 = 3032 / (3032 + 252)\n",
    "sens1 = 3051 / (3051 + 265)\n",
    "print('Best Specificity:',spec1, 'Best Sensitivity:', sens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pickle.load(open('./assets/forest_confusionmatrix.pkl','rb'))\n",
    "plt.title('\"Forest\" model Confusion Matrix');\n",
    "\n",
    "# plt.savefig('imgs/forestconf.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forest model seemed to optimize for sensitivity which is something to note in random forest models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest Specificity: 0.8982947624847747 Forest Sensitivity: 0.9351628468033776\n"
     ]
    }
   ],
   "source": [
    "spec2 = 2950 / (2950 + 334)\n",
    "sens2 = 3101 / (3101 + 215)\n",
    "print('Forest Specificity:',spec2, 'Forest Sensitivity:', sens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pickle.load(open('./assets/another_confusionmatrix.pkl','rb'))\n",
    "plt.title('\"Another\" model Confusion Matrix');\n",
    "\n",
    "# plt.savefig('imgs/anotherconf.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity was also optimized for in the `another` model without having to decrease the sensitivity as much. In comparison to `best` model, I thought this would be a good thing to point out because I was able to decrease the amount of false negatives. So if my model was used to try and figure out if someone had depresion (*in an extreme case where my model is the only thing that can determine that*) through their reddit post, at the very least we are reducing the amount of times we make mistakes. Ideally, we want false negatives to be 0, but to get my sensitivity this high was a good thing to notice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another Specificity: 0.9147381242387332 Another Sensitivity: 0.9484318455971049\n"
     ]
    }
   ],
   "source": [
    "spec3 = 3004 / (3004 + 280)\n",
    "sens3 = 3145 / (3145 + 171)\n",
    "print('Another Specificity:',spec3, 'Another Sensitivity:', sens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-EDA Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "---\n",
    "We will see what were the top features from each model and the weight they brought in indicating whether or not a post was from the \"depression\" or \"shower thoughts\" subreddit.\n",
    "\n",
    "How I was able to interpret the coefficients on my logistic regression was from [here](https://www.displayr.com/how-to-interpret-logistic-regression-coefficients/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word(s)</th>\n",
       "      <th>coefficient</th>\n",
       "      <th>exp_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>depress</td>\n",
       "      <td>9.040511</td>\n",
       "      <td>8438.084964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>my</td>\n",
       "      <td>5.688897</td>\n",
       "      <td>295.567368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>me</td>\n",
       "      <td>5.258693</td>\n",
       "      <td>192.229999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>feel</td>\n",
       "      <td>5.007686</td>\n",
       "      <td>149.558261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>suicid</td>\n",
       "      <td>3.840616</td>\n",
       "      <td>46.554127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>help</td>\n",
       "      <td>3.816397</td>\n",
       "      <td>45.440198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>myself</td>\n",
       "      <td>3.583370</td>\n",
       "      <td>35.994655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>anyon</td>\n",
       "      <td>2.918018</td>\n",
       "      <td>18.504566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>is it</td>\n",
       "      <td>2.739656</td>\n",
       "      <td>15.481652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>feel like</td>\n",
       "      <td>2.697899</td>\n",
       "      <td>14.848507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725</th>\n",
       "      <td>want to</td>\n",
       "      <td>2.660107</td>\n",
       "      <td>14.297823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>wish</td>\n",
       "      <td>2.628148</td>\n",
       "      <td>13.848098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>friend</td>\n",
       "      <td>2.547431</td>\n",
       "      <td>12.774250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>tire</td>\n",
       "      <td>2.481849</td>\n",
       "      <td>11.963363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>mental</td>\n",
       "      <td>2.472815</td>\n",
       "      <td>11.855772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>im</td>\n",
       "      <td>2.458320</td>\n",
       "      <td>11.685165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>sad</td>\n",
       "      <td>2.432114</td>\n",
       "      <td>11.382917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>therapi</td>\n",
       "      <td>2.426240</td>\n",
       "      <td>11.316249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>happi</td>\n",
       "      <td>2.308422</td>\n",
       "      <td>10.058536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>advic</td>\n",
       "      <td>2.259658</td>\n",
       "      <td>9.579815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>struggl</td>\n",
       "      <td>2.173840</td>\n",
       "      <td>8.791984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>fuck</td>\n",
       "      <td>2.138048</td>\n",
       "      <td>8.482860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>vent</td>\n",
       "      <td>2.137042</td>\n",
       "      <td>8.474333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>want</td>\n",
       "      <td>2.120504</td>\n",
       "      <td>8.335341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>hate</td>\n",
       "      <td>2.113494</td>\n",
       "      <td>8.277107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word(s)  coefficient     exp_coef\n",
       "592     depress     9.040511  8438.084964\n",
       "1603         my     5.688897   295.567368\n",
       "1517         me     5.258693   192.229999\n",
       "819        feel     5.007686   149.558261\n",
       "2254     suicid     3.840616    46.554127\n",
       "1066       help     3.816397    45.440198\n",
       "1630     myself     3.583370    35.994655\n",
       "165       anyon     2.918018    18.504566\n",
       "1233      is it     2.739656    15.481652\n",
       "827   feel like     2.697899    14.848507\n",
       "2725    want to     2.660107    14.297823\n",
       "2856       wish     2.628148    13.848098\n",
       "904      friend     2.547431    12.774250\n",
       "2525       tire     2.481849    11.963363\n",
       "1540     mental     2.472815    11.855772\n",
       "1148         im     2.458320    11.685165\n",
       "2035        sad     2.432114    11.382917\n",
       "2443    therapi     2.426240    11.316249\n",
       "1021      happi     2.308422    10.058536\n",
       "56        advic     2.259658     9.579815\n",
       "2237    struggl     2.173840     8.791984\n",
       "916        fuck     2.138048     8.482860\n",
       "2699       vent     2.137042     8.474333\n",
       "2724       want     2.120504     8.335341\n",
       "1029       hate     2.113494     8.277107"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First create the DataFrame\n",
    "best_df = pd.DataFrame(best.best_estimator_.named_steps.model.coef_, \n",
    "             columns = best.best_estimator_.named_steps.trans.get_feature_names(),\n",
    "             ).T\n",
    "# Reset the index\n",
    "best_df.reset_index(inplace=True)\n",
    "# Rename the column names\n",
    "best_df.rename(columns={'index':'word(s)', \n",
    "                        0: 'coefficient'}, inplace=True)\n",
    "# Sort Values \n",
    "ordered_best = best_df.sort_values('coefficient',ascending=False)\n",
    "# Add Exponentiated Coefficients\n",
    "ordered_best['exp_coef'] = np.exp(ordered_best['coefficient'])\n",
    "# Look at the top and bottom\n",
    "ordered_best.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word(s)</th>\n",
       "      <th>coefficient</th>\n",
       "      <th>exp_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>movi</td>\n",
       "      <td>-2.418814</td>\n",
       "      <td>0.089027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>are just</td>\n",
       "      <td>-2.447682</td>\n",
       "      <td>0.086494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>car</td>\n",
       "      <td>-2.467881</td>\n",
       "      <td>0.084764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>technic</td>\n",
       "      <td>-2.485573</td>\n",
       "      <td>0.083278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>water</td>\n",
       "      <td>-2.501101</td>\n",
       "      <td>0.081995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>ha</td>\n",
       "      <td>-2.503127</td>\n",
       "      <td>0.081829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>you can</td>\n",
       "      <td>-2.507241</td>\n",
       "      <td>0.081493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>mask</td>\n",
       "      <td>-2.534510</td>\n",
       "      <td>0.079301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>human</td>\n",
       "      <td>-2.545687</td>\n",
       "      <td>0.078419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>would</td>\n",
       "      <td>-2.569308</td>\n",
       "      <td>0.076589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>mean</td>\n",
       "      <td>-2.680829</td>\n",
       "      <td>0.068506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2683</th>\n",
       "      <td>us</td>\n",
       "      <td>-2.722377</td>\n",
       "      <td>0.065718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>our</td>\n",
       "      <td>-2.831342</td>\n",
       "      <td>0.058934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>is just</td>\n",
       "      <td>-2.896528</td>\n",
       "      <td>0.055215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>probabl</td>\n",
       "      <td>-3.573763</td>\n",
       "      <td>0.028050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>than</td>\n",
       "      <td>-3.809563</td>\n",
       "      <td>0.022158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2981</th>\n",
       "      <td>your</td>\n",
       "      <td>-3.928658</td>\n",
       "      <td>0.019670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>they</td>\n",
       "      <td>-4.034283</td>\n",
       "      <td>0.017698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>you</td>\n",
       "      <td>-4.231481</td>\n",
       "      <td>0.014531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>their</td>\n",
       "      <td>-4.265702</td>\n",
       "      <td>0.014042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>if</td>\n",
       "      <td>-4.864676</td>\n",
       "      <td>0.007714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>the</td>\n",
       "      <td>-5.075598</td>\n",
       "      <td>0.006247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>we</td>\n",
       "      <td>-5.159189</td>\n",
       "      <td>0.005746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>are</td>\n",
       "      <td>-5.772286</td>\n",
       "      <td>0.003113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>is</td>\n",
       "      <td>-6.791752</td>\n",
       "      <td>0.001123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word(s)  coefficient  exp_coef\n",
       "1592      movi    -2.418814  0.089027\n",
       "186   are just    -2.447682  0.086494\n",
       "439        car    -2.467881  0.084764\n",
       "2282   technic    -2.485573  0.083278\n",
       "2743     water    -2.501101  0.081995\n",
       "999         ha    -2.503127  0.081829\n",
       "2929   you can    -2.507241  0.081493\n",
       "1506      mask    -2.534510  0.079301\n",
       "1115     human    -2.545687  0.078419\n",
       "2895     would    -2.569308  0.076589\n",
       "1524      mean    -2.680829  0.068506\n",
       "2683        us    -2.722377  0.065718\n",
       "1795       our    -2.831342  0.058934\n",
       "1234   is just    -2.896528  0.055215\n",
       "1920   probabl    -3.573763  0.028050\n",
       "2298      than    -3.809563  0.022158\n",
       "2981      your    -3.928658  0.019670\n",
       "2458      they    -4.034283  0.017698\n",
       "2924       you    -4.231481  0.014531\n",
       "2432     their    -4.265702  0.014042\n",
       "1125        if    -4.864676  0.007714\n",
       "2332       the    -5.075598  0.006247\n",
       "2750        we    -5.159189  0.005746\n",
       "178        are    -5.772286  0.003113\n",
       "1218        is    -6.791752  0.001123"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_best.tail(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word(s)</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>is</td>\n",
       "      <td>0.025010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7480</th>\n",
       "      <td>the</td>\n",
       "      <td>0.024528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>are</td>\n",
       "      <td>0.020807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>my</td>\n",
       "      <td>0.014939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8488</th>\n",
       "      <td>you</td>\n",
       "      <td>0.014808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>depress</td>\n",
       "      <td>0.014066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>if</td>\n",
       "      <td>0.012990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>to</td>\n",
       "      <td>0.010490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8226</th>\n",
       "      <td>we</td>\n",
       "      <td>0.010063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>of</td>\n",
       "      <td>0.009416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>feel</td>\n",
       "      <td>0.009279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3788</th>\n",
       "      <td>in</td>\n",
       "      <td>0.009089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>me</td>\n",
       "      <td>0.008779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8494</th>\n",
       "      <td>your</td>\n",
       "      <td>0.007015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>and</td>\n",
       "      <td>0.006797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>just</td>\n",
       "      <td>0.006442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>it</td>\n",
       "      <td>0.006384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>do</td>\n",
       "      <td>0.006307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>be</td>\n",
       "      <td>0.006307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7477</th>\n",
       "      <td>that</td>\n",
       "      <td>0.006141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7505</th>\n",
       "      <td>they</td>\n",
       "      <td>0.005546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5512</th>\n",
       "      <td>peopl</td>\n",
       "      <td>0.005289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5852</th>\n",
       "      <td>probabl</td>\n",
       "      <td>0.005151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384</th>\n",
       "      <td>like</td>\n",
       "      <td>0.004702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8398</th>\n",
       "      <td>would</td>\n",
       "      <td>0.004670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word(s)  feature_importance\n",
       "3990       is            0.025010\n",
       "7480      the            0.024528\n",
       "632       are            0.020807\n",
       "4955       my            0.014939\n",
       "8488      you            0.014808\n",
       "2107  depress            0.014066\n",
       "3734       if            0.012990\n",
       "7606       to            0.010490\n",
       "8226       we            0.010063\n",
       "5206       of            0.009416\n",
       "2859     feel            0.009279\n",
       "3788       in            0.009089\n",
       "4631       me            0.008779\n",
       "8494     your            0.007015\n",
       "535       and            0.006797\n",
       "4120     just            0.006442\n",
       "4004       it            0.006384\n",
       "2302       do            0.006307\n",
       "877        be            0.006307\n",
       "7477     that            0.006141\n",
       "7505     they            0.005546\n",
       "5512    peopl            0.005289\n",
       "5852  probabl            0.005151\n",
       "4384     like            0.004702\n",
       "8398    would            0.004670"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forest model\n",
    "forest_df = pd.DataFrame(forest.best_estimator_.named_steps.model.feature_importances_, \n",
    "             index = forest.best_estimator_.named_steps.trans.get_feature_names(),\n",
    "             )\n",
    "# Reset Index\n",
    "forest_df.reset_index(inplace=True)\n",
    "# Rename columns\n",
    "forest_df.rename(columns={'index':'word(s)',\n",
    "                         0:'feature_importance'}, inplace=True)\n",
    "# Sort Values\n",
    "forest_df = forest_df.sort_values('feature_importance',ascending=False)\n",
    "# View top 25 then bottom 25\n",
    "forest_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word(s)</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6375</th>\n",
       "      <td>romantic</td>\n",
       "      <td>2.018436e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>divid</td>\n",
       "      <td>1.409999e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5383</th>\n",
       "      <td>painless</td>\n",
       "      <td>1.467162e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4628</th>\n",
       "      <td>mcu</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>disproven</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3350</th>\n",
       "      <td>gringo</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>budapest</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3639</th>\n",
       "      <td>host</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6033</th>\n",
       "      <td>radiu</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4426</th>\n",
       "      <td>locker</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>amongst</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5082</th>\n",
       "      <td>nilli</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>impati</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>soda</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>62</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4320</th>\n",
       "      <td>leftov</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>bust</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>48</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>pharmaceut</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>feez</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3913</th>\n",
       "      <td>inteleginc</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>breastfe</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6165</th>\n",
       "      <td>reign</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1955</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4690</th>\n",
       "      <td>mere</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word(s)  feature_importance\n",
       "6375    romantic        2.018436e-09\n",
       "2292       divid        1.409999e-09\n",
       "5383    painless        1.467162e-10\n",
       "4628         mcu        0.000000e+00\n",
       "2271   disproven        0.000000e+00\n",
       "3350      gringo        0.000000e+00\n",
       "1185    budapest        0.000000e+00\n",
       "3639        host        0.000000e+00\n",
       "6033       radiu        0.000000e+00\n",
       "4426      locker        0.000000e+00\n",
       "515      amongst        0.000000e+00\n",
       "5082       nilli        0.000000e+00\n",
       "3770      impati        0.000000e+00\n",
       "6916        soda        0.000000e+00\n",
       "212           62        0.000000e+00\n",
       "4320      leftov        0.000000e+00\n",
       "1227        bust        0.000000e+00\n",
       "184           48        0.000000e+00\n",
       "5556  pharmaceut        0.000000e+00\n",
       "2861        feez        0.000000e+00\n",
       "3913  inteleginc        0.000000e+00\n",
       "1137    breastfe        0.000000e+00\n",
       "6165       reign        0.000000e+00\n",
       "62          1955        0.000000e+00\n",
       "4690        mere        0.000000e+00"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_df.tail(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word(s)</th>\n",
       "      <th>coefficient</th>\n",
       "      <th>exp_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>depress</td>\n",
       "      <td>4.355729</td>\n",
       "      <td>77.923585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4463</th>\n",
       "      <td>suicid</td>\n",
       "      <td>2.613901</td>\n",
       "      <td>13.652203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>mental</td>\n",
       "      <td>2.221862</td>\n",
       "      <td>9.224495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>help</td>\n",
       "      <td>2.024210</td>\n",
       "      <td>7.570128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>is it</td>\n",
       "      <td>1.966582</td>\n",
       "      <td>7.146206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490</th>\n",
       "      <td>is there</td>\n",
       "      <td>1.848365</td>\n",
       "      <td>6.349430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3003</th>\n",
       "      <td>me</td>\n",
       "      <td>1.832939</td>\n",
       "      <td>6.252236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>advic</td>\n",
       "      <td>1.816785</td>\n",
       "      <td>6.152050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>wish</td>\n",
       "      <td>1.752979</td>\n",
       "      <td>5.771771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5398</th>\n",
       "      <td>vent</td>\n",
       "      <td>1.748445</td>\n",
       "      <td>5.745664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3220</th>\n",
       "      <td>myself</td>\n",
       "      <td>1.714140</td>\n",
       "      <td>5.551901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>hardest</td>\n",
       "      <td>1.713246</td>\n",
       "      <td>5.546936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>therapi</td>\n",
       "      <td>1.696625</td>\n",
       "      <td>5.455505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>antidepress</td>\n",
       "      <td>1.693320</td>\n",
       "      <td>5.437502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5036</th>\n",
       "      <td>tire</td>\n",
       "      <td>1.669223</td>\n",
       "      <td>5.308040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>my</td>\n",
       "      <td>1.666745</td>\n",
       "      <td>5.294904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2491</th>\n",
       "      <td>is thi</td>\n",
       "      <td>1.623288</td>\n",
       "      <td>5.069735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4459</th>\n",
       "      <td>suffer</td>\n",
       "      <td>1.609934</td>\n",
       "      <td>5.002480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4439</th>\n",
       "      <td>struggl</td>\n",
       "      <td>1.609930</td>\n",
       "      <td>5.002463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>feel</td>\n",
       "      <td>1.597934</td>\n",
       "      <td>4.942811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>sad</td>\n",
       "      <td>1.582223</td>\n",
       "      <td>4.865761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>lone</td>\n",
       "      <td>1.559306</td>\n",
       "      <td>4.755518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>hey</td>\n",
       "      <td>1.539028</td>\n",
       "      <td>4.660059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>im</td>\n",
       "      <td>1.530358</td>\n",
       "      <td>4.619830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5846</th>\n",
       "      <td>you are</td>\n",
       "      <td>1.516685</td>\n",
       "      <td>4.557091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word(s)  coefficient   exp_coef\n",
       "1144      depress     4.355729  77.923585\n",
       "4463       suicid     2.613901  13.652203\n",
       "3054       mental     2.221862   9.224495\n",
       "2078         help     2.024210   7.570128\n",
       "2432        is it     1.966582   7.146206\n",
       "2490     is there     1.848365   6.349430\n",
       "3003           me     1.832939   6.252236\n",
       "109         advic     1.816785   6.152050\n",
       "5715         wish     1.752979   5.771771\n",
       "5398         vent     1.748445   5.745664\n",
       "3220       myself     1.714140   5.551901\n",
       "1990      hardest     1.713246   5.546936\n",
       "4871      therapi     1.696625   5.455505\n",
       "331   antidepress     1.693320   5.437502\n",
       "5036         tire     1.669223   5.308040\n",
       "3167           my     1.666745   5.294904\n",
       "2491       is thi     1.623288   5.069735\n",
       "4459       suffer     1.609934   5.002480\n",
       "4439      struggl     1.609930   5.002463\n",
       "1593         feel     1.597934   4.942811\n",
       "4076          sad     1.582223   4.865761\n",
       "2875         lone     1.559306   4.755518\n",
       "2090          hey     1.539028   4.660059\n",
       "2253           im     1.530358   4.619830\n",
       "5846      you are     1.516685   4.557091"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First create the DataFrame\n",
    "another_df = pd.DataFrame(another.best_estimator_.named_steps.model.coef_, \n",
    "             columns = another.best_estimator_.named_steps.trans.get_feature_names(),\n",
    "             ).T\n",
    "# Reset the index\n",
    "another_df.reset_index(inplace=True)\n",
    "# Rename the column names\n",
    "another_df.rename(columns={'index':'word(s)', \n",
    "                        0: 'coefficient'}, inplace=True)\n",
    "# Sort Values \n",
    "another_df = another_df.sort_values('coefficient',ascending=False)\n",
    "# Add Exponentiated Coefficients\n",
    "another_df['exp_coef'] = np.exp(another_df['coefficient'])\n",
    "# Look at the top and bottom\n",
    "another_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word(s)</th>\n",
       "      <th>coefficient</th>\n",
       "      <th>exp_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>gay</td>\n",
       "      <td>-1.458247</td>\n",
       "      <td>0.232644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>cannib</td>\n",
       "      <td>-1.464206</td>\n",
       "      <td>0.231262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>call</td>\n",
       "      <td>-1.472269</td>\n",
       "      <td>0.229404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3052</th>\n",
       "      <td>men</td>\n",
       "      <td>-1.501767</td>\n",
       "      <td>0.222736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>hair</td>\n",
       "      <td>-1.508528</td>\n",
       "      <td>0.221235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4183</th>\n",
       "      <td>shark</td>\n",
       "      <td>-1.513356</td>\n",
       "      <td>0.220170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>must</td>\n",
       "      <td>-1.530992</td>\n",
       "      <td>0.216321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>cooki</td>\n",
       "      <td>-1.597107</td>\n",
       "      <td>0.202481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>car</td>\n",
       "      <td>-1.648539</td>\n",
       "      <td>0.192331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>we</td>\n",
       "      <td>-1.651022</td>\n",
       "      <td>0.191854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4554</th>\n",
       "      <td>than</td>\n",
       "      <td>-1.660666</td>\n",
       "      <td>0.190012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142</th>\n",
       "      <td>movi</td>\n",
       "      <td>-1.661192</td>\n",
       "      <td>0.189912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>mask</td>\n",
       "      <td>-1.672751</td>\n",
       "      <td>0.187730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>fuck my</td>\n",
       "      <td>-1.679904</td>\n",
       "      <td>0.186392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>is just</td>\n",
       "      <td>-1.699424</td>\n",
       "      <td>0.182789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>are just</td>\n",
       "      <td>-1.705797</td>\n",
       "      <td>0.181628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831</th>\n",
       "      <td>probabl</td>\n",
       "      <td>-1.788403</td>\n",
       "      <td>0.167227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>fuck my life</td>\n",
       "      <td>-1.810130</td>\n",
       "      <td>0.163633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5485</th>\n",
       "      <td>water</td>\n",
       "      <td>-1.821982</td>\n",
       "      <td>0.161705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4526</th>\n",
       "      <td>technic</td>\n",
       "      <td>-1.834058</td>\n",
       "      <td>0.159764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>dog</td>\n",
       "      <td>-1.844506</td>\n",
       "      <td>0.158103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4838</th>\n",
       "      <td>their</td>\n",
       "      <td>-2.012811</td>\n",
       "      <td>0.133613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2684</th>\n",
       "      <td>karma</td>\n",
       "      <td>-2.023461</td>\n",
       "      <td>0.132197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2394</th>\n",
       "      <td>is</td>\n",
       "      <td>-2.038757</td>\n",
       "      <td>0.130190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>are</td>\n",
       "      <td>-2.196404</td>\n",
       "      <td>0.111202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word(s)  coefficient  exp_coef\n",
       "1815           gay    -1.458247  0.232644\n",
       "865         cannib    -1.464206  0.231262\n",
       "811           call    -1.472269  0.229404\n",
       "3052           men    -1.501767  0.222736\n",
       "1963          hair    -1.508528  0.221235\n",
       "4183         shark    -1.513356  0.220170\n",
       "3164          must    -1.530992  0.216321\n",
       "1018         cooki    -1.597107  0.202481\n",
       "871            car    -1.648539  0.192331\n",
       "5495            we    -1.651022  0.191854\n",
       "4554          than    -1.660666  0.190012\n",
       "3142          movi    -1.661192  0.189912\n",
       "2982          mask    -1.672751  0.187730\n",
       "1786       fuck my    -1.679904  0.186392\n",
       "2438       is just    -1.699424  0.182789\n",
       "383       are just    -1.705797  0.181628\n",
       "3831       probabl    -1.788403  0.167227\n",
       "1787  fuck my life    -1.810130  0.163633\n",
       "5485         water    -1.821982  0.161705\n",
       "4526       technic    -1.834058  0.159764\n",
       "1286           dog    -1.844506  0.158103\n",
       "4838         their    -2.012811  0.133613\n",
       "2684         karma    -2.023461  0.132197\n",
       "2394            is    -2.038757  0.130190\n",
       "364            are    -2.196404  0.111202"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_df.tail(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that when it came to figuring out if a post was from the \"depression\" subreddit, it was clearly obvious that many variants of the word ***depress*** would be a top consideration for that. What was surprising was in the `another` model the combination of <font color = 'red'>\"*fuck my life*\"</font> and <font color = 'red'>\"*fuck my*\"</font> were in the bottom in terms of coefficient. This meant that these words were not from the \"depression\" subreddit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which words are good indicators outside of the stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made a formula that could help me filter out the stopword so I can focus on words that have more meaning in determining whether or not a post is from the \"depression\" subreddit or the \"shower thoughts\" subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word(word):\n",
    "    return word in stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word(s)</th>\n",
       "      <th>coefficient</th>\n",
       "      <th>exp_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>depress</td>\n",
       "      <td>9.040511</td>\n",
       "      <td>8438.084964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>feel</td>\n",
       "      <td>5.007686</td>\n",
       "      <td>149.558261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>suicid</td>\n",
       "      <td>3.840616</td>\n",
       "      <td>46.554127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>help</td>\n",
       "      <td>3.816397</td>\n",
       "      <td>45.440198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>anyon</td>\n",
       "      <td>2.918018</td>\n",
       "      <td>18.504566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>is it</td>\n",
       "      <td>2.739656</td>\n",
       "      <td>15.481652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>feel like</td>\n",
       "      <td>2.697899</td>\n",
       "      <td>14.848507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725</th>\n",
       "      <td>want to</td>\n",
       "      <td>2.660107</td>\n",
       "      <td>14.297823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>wish</td>\n",
       "      <td>2.628148</td>\n",
       "      <td>13.848098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>friend</td>\n",
       "      <td>2.547431</td>\n",
       "      <td>12.774250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>tire</td>\n",
       "      <td>2.481849</td>\n",
       "      <td>11.963363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>mental</td>\n",
       "      <td>2.472815</td>\n",
       "      <td>11.855772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>im</td>\n",
       "      <td>2.458320</td>\n",
       "      <td>11.685165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>sad</td>\n",
       "      <td>2.432114</td>\n",
       "      <td>11.382917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>therapi</td>\n",
       "      <td>2.426240</td>\n",
       "      <td>11.316249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>happi</td>\n",
       "      <td>2.308422</td>\n",
       "      <td>10.058536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>advic</td>\n",
       "      <td>2.259658</td>\n",
       "      <td>9.579815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>struggl</td>\n",
       "      <td>2.173840</td>\n",
       "      <td>8.791984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>fuck</td>\n",
       "      <td>2.138048</td>\n",
       "      <td>8.482860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>vent</td>\n",
       "      <td>2.137042</td>\n",
       "      <td>8.474333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>want</td>\n",
       "      <td>2.120504</td>\n",
       "      <td>8.335341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>hate</td>\n",
       "      <td>2.113494</td>\n",
       "      <td>8.277107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>cri</td>\n",
       "      <td>2.109227</td>\n",
       "      <td>8.241871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>antidepress</td>\n",
       "      <td>2.070648</td>\n",
       "      <td>7.929962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>lone</td>\n",
       "      <td>1.985056</td>\n",
       "      <td>7.279452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word(s)  coefficient     exp_coef\n",
       "592       depress     9.040511  8438.084964\n",
       "819          feel     5.007686   149.558261\n",
       "2254       suicid     3.840616    46.554127\n",
       "1066         help     3.816397    45.440198\n",
       "165         anyon     2.918018    18.504566\n",
       "1233        is it     2.739656    15.481652\n",
       "827     feel like     2.697899    14.848507\n",
       "2725      want to     2.660107    14.297823\n",
       "2856         wish     2.628148    13.848098\n",
       "904        friend     2.547431    12.774250\n",
       "2525         tire     2.481849    11.963363\n",
       "1540       mental     2.472815    11.855772\n",
       "1148           im     2.458320    11.685165\n",
       "2035          sad     2.432114    11.382917\n",
       "2443      therapi     2.426240    11.316249\n",
       "1021        happi     2.308422    10.058536\n",
       "56          advic     2.259658     9.579815\n",
       "2237      struggl     2.173840     8.791984\n",
       "916          fuck     2.138048     8.482860\n",
       "2699         vent     2.137042     8.474333\n",
       "2724         want     2.120504     8.335341\n",
       "1029         hate     2.113494     8.277107\n",
       "552           cri     2.109227     8.241871\n",
       "160   antidepress     2.070648     7.929962\n",
       "1453         lone     1.985056     7.279452"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_best[ordered_best['word(s)'].map(check_word)==False].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word(s)</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>depress</td>\n",
       "      <td>0.014066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>feel</td>\n",
       "      <td>0.009279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5512</th>\n",
       "      <td>peopl</td>\n",
       "      <td>0.005289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5852</th>\n",
       "      <td>probabl</td>\n",
       "      <td>0.005151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384</th>\n",
       "      <td>like</td>\n",
       "      <td>0.004702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8398</th>\n",
       "      <td>would</td>\n",
       "      <td>0.004670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4369</th>\n",
       "      <td>life</td>\n",
       "      <td>0.004392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3522</th>\n",
       "      <td>help</td>\n",
       "      <td>0.004317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>want</td>\n",
       "      <td>0.004280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7508</th>\n",
       "      <td>thi</td>\n",
       "      <td>0.004213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581</th>\n",
       "      <td>time</td>\n",
       "      <td>0.003069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3670</th>\n",
       "      <td>human</td>\n",
       "      <td>0.003037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>know</td>\n",
       "      <td>0.002867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>anyon</td>\n",
       "      <td>0.002816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7991</th>\n",
       "      <td>us</td>\n",
       "      <td>0.002741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015</th>\n",
       "      <td>need</td>\n",
       "      <td>0.002686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3204</th>\n",
       "      <td>get</td>\n",
       "      <td>0.002680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>call</td>\n",
       "      <td>0.002624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5242</th>\n",
       "      <td>one</td>\n",
       "      <td>0.002518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>fuck</td>\n",
       "      <td>0.002503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>tire</td>\n",
       "      <td>0.002498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7252</th>\n",
       "      <td>suicid</td>\n",
       "      <td>0.002475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8290</th>\n",
       "      <td>whi</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>friend</td>\n",
       "      <td>0.002359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3244</th>\n",
       "      <td>go</td>\n",
       "      <td>0.002354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word(s)  feature_importance\n",
       "2107  depress            0.014066\n",
       "2859     feel            0.009279\n",
       "5512    peopl            0.005289\n",
       "5852  probabl            0.005151\n",
       "4384     like            0.004702\n",
       "8398    would            0.004670\n",
       "4369     life            0.004392\n",
       "3522     help            0.004317\n",
       "8187     want            0.004280\n",
       "7508      thi            0.004213\n",
       "7581     time            0.003069\n",
       "3670    human            0.003037\n",
       "4208     know            0.002867\n",
       "583     anyon            0.002816\n",
       "7991       us            0.002741\n",
       "5015     need            0.002686\n",
       "3204      get            0.002680\n",
       "1262     call            0.002624\n",
       "5242      one            0.002518\n",
       "3100     fuck            0.002503\n",
       "7594     tire            0.002498\n",
       "7252   suicid            0.002475\n",
       "8290      whi            0.002400\n",
       "3072   friend            0.002359\n",
       "3244       go            0.002354"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_df[forest_df['word(s)'].map(check_word)==False].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word(s)</th>\n",
       "      <th>coefficient</th>\n",
       "      <th>exp_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>depress</td>\n",
       "      <td>4.355729</td>\n",
       "      <td>77.923585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4463</th>\n",
       "      <td>suicid</td>\n",
       "      <td>2.613901</td>\n",
       "      <td>13.652203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>mental</td>\n",
       "      <td>2.221862</td>\n",
       "      <td>9.224495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>help</td>\n",
       "      <td>2.024210</td>\n",
       "      <td>7.570128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>is it</td>\n",
       "      <td>1.966582</td>\n",
       "      <td>7.146206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490</th>\n",
       "      <td>is there</td>\n",
       "      <td>1.848365</td>\n",
       "      <td>6.349430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>advic</td>\n",
       "      <td>1.816785</td>\n",
       "      <td>6.152050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>wish</td>\n",
       "      <td>1.752979</td>\n",
       "      <td>5.771771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5398</th>\n",
       "      <td>vent</td>\n",
       "      <td>1.748445</td>\n",
       "      <td>5.745664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>hardest</td>\n",
       "      <td>1.713246</td>\n",
       "      <td>5.546936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>therapi</td>\n",
       "      <td>1.696625</td>\n",
       "      <td>5.455505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>antidepress</td>\n",
       "      <td>1.693320</td>\n",
       "      <td>5.437502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5036</th>\n",
       "      <td>tire</td>\n",
       "      <td>1.669223</td>\n",
       "      <td>5.308040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2491</th>\n",
       "      <td>is thi</td>\n",
       "      <td>1.623288</td>\n",
       "      <td>5.069735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4459</th>\n",
       "      <td>suffer</td>\n",
       "      <td>1.609934</td>\n",
       "      <td>5.002480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4439</th>\n",
       "      <td>struggl</td>\n",
       "      <td>1.609930</td>\n",
       "      <td>5.002463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>feel</td>\n",
       "      <td>1.597934</td>\n",
       "      <td>4.942811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>sad</td>\n",
       "      <td>1.582223</td>\n",
       "      <td>4.865761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>lone</td>\n",
       "      <td>1.559306</td>\n",
       "      <td>4.755518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>hey</td>\n",
       "      <td>1.539028</td>\n",
       "      <td>4.660059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>im</td>\n",
       "      <td>1.530358</td>\n",
       "      <td>4.619830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5846</th>\n",
       "      <td>you are</td>\n",
       "      <td>1.516685</td>\n",
       "      <td>4.557091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>anyon</td>\n",
       "      <td>1.502526</td>\n",
       "      <td>4.493022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4902</th>\n",
       "      <td>they are</td>\n",
       "      <td>1.499180</td>\n",
       "      <td>4.478014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>for you</td>\n",
       "      <td>1.471689</td>\n",
       "      <td>4.356589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word(s)  coefficient   exp_coef\n",
       "1144      depress     4.355729  77.923585\n",
       "4463       suicid     2.613901  13.652203\n",
       "3054       mental     2.221862   9.224495\n",
       "2078         help     2.024210   7.570128\n",
       "2432        is it     1.966582   7.146206\n",
       "2490     is there     1.848365   6.349430\n",
       "109         advic     1.816785   6.152050\n",
       "5715         wish     1.752979   5.771771\n",
       "5398         vent     1.748445   5.745664\n",
       "1990      hardest     1.713246   5.546936\n",
       "4871      therapi     1.696625   5.455505\n",
       "331   antidepress     1.693320   5.437502\n",
       "5036         tire     1.669223   5.308040\n",
       "2491       is thi     1.623288   5.069735\n",
       "4459       suffer     1.609934   5.002480\n",
       "4439      struggl     1.609930   5.002463\n",
       "1593         feel     1.597934   4.942811\n",
       "4076          sad     1.582223   4.865761\n",
       "2875         lone     1.559306   4.755518\n",
       "2090          hey     1.539028   4.660059\n",
       "2253           im     1.530358   4.619830\n",
       "5846      you are     1.516685   4.557091\n",
       "337         anyon     1.502526   4.493022\n",
       "4902     they are     1.499180   4.478014\n",
       "1742      for you     1.471689   4.356589"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_df[another_df['word(s)'].map(check_word)==False].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tough thing to interpret is some of the root words. Since the models I selected were PorterStemmed words, some words look \"funky\" but are still readable. \n",
    "\n",
    "It seems that even if there happy like words such as \"friend\" or \"happy\" that became a good indicator if the word was from the depression subreddit.\n",
    "\n",
    "\"Suicide\" seems to be an also recurring top word to help determine if a person posted in the depression subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How sad are they?\n",
    "\n",
    "---\n",
    "When people decide to post a very sad or depressing post about their emotions, their language tends to be negative. Whether it is emitting pure sadness or pure anger, the sentiment is there. \n",
    "\n",
    "I will use the built in `SentimentIntensityAnalyzer` and group by neutral, positive, or negative to determine how depressing a post can be from the depression subreddit but also see if there is a noticable difference from the shower thoughts subreddit.\n",
    "\n",
    "My assumption is that shower thoughts is all over, leaning heavily on neutral while depression should be wallowing in negative sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the WCBC Week 5 Advanced piping notes\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "# Write a function to get the compound sentiment scores for a post\n",
    "def get_compound_sentiment(post):\n",
    "    return sia.polarity_scores(post)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_best['compound'] = ordered_best['word(s)'].apply(get_compound_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word(s)</th>\n",
       "      <th>coefficient</th>\n",
       "      <th>exp_coef</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>depress</td>\n",
       "      <td>9.040511</td>\n",
       "      <td>8438.084964</td>\n",
       "      <td>-0.4939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>sad</td>\n",
       "      <td>2.432114</td>\n",
       "      <td>11.382917</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>fuck</td>\n",
       "      <td>2.138048</td>\n",
       "      <td>8.482860</td>\n",
       "      <td>-0.5423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>hate</td>\n",
       "      <td>2.113494</td>\n",
       "      <td>8.277107</td>\n",
       "      <td>-0.5719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>lone</td>\n",
       "      <td>1.985056</td>\n",
       "      <td>7.279452</td>\n",
       "      <td>-0.2732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>rant</td>\n",
       "      <td>1.674188</td>\n",
       "      <td>5.334463</td>\n",
       "      <td>-0.3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>suffer</td>\n",
       "      <td>1.634686</td>\n",
       "      <td>5.127846</td>\n",
       "      <td>-0.5423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>hurt</td>\n",
       "      <td>1.590016</td>\n",
       "      <td>4.903826</td>\n",
       "      <td>-0.5267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>lost</td>\n",
       "      <td>1.476297</td>\n",
       "      <td>4.376709</td>\n",
       "      <td>-0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>kill myself</td>\n",
       "      <td>1.409631</td>\n",
       "      <td>4.094443</td>\n",
       "      <td>-0.6908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word(s)  coefficient     exp_coef  compound\n",
       "592       depress     9.040511  8438.084964   -0.4939\n",
       "2035          sad     2.432114    11.382917   -0.4767\n",
       "916          fuck     2.138048     8.482860   -0.5423\n",
       "1029         hate     2.113494     8.277107   -0.5719\n",
       "1453         lone     1.985056     7.279452   -0.2732\n",
       "1950         rant     1.674188     5.334463   -0.3400\n",
       "2251       suffer     1.634686     5.127846   -0.5423\n",
       "1118         hurt     1.590016     4.903826   -0.5267\n",
       "1468         lost     1.476297     4.376709   -0.3182\n",
       "1364  kill myself     1.409631     4.094443   -0.6908"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_best[ordered_best['compound'] < 0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word(s)</th>\n",
       "      <th>coefficient</th>\n",
       "      <th>exp_coef</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>help</td>\n",
       "      <td>3.816397</td>\n",
       "      <td>45.440198</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>feel like</td>\n",
       "      <td>2.697899</td>\n",
       "      <td>14.848507</td>\n",
       "      <td>0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725</th>\n",
       "      <td>want to</td>\n",
       "      <td>2.660107</td>\n",
       "      <td>14.297823</td>\n",
       "      <td>0.0772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>wish</td>\n",
       "      <td>2.628148</td>\n",
       "      <td>13.848098</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>friend</td>\n",
       "      <td>2.547431</td>\n",
       "      <td>12.774250</td>\n",
       "      <td>0.4939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>want</td>\n",
       "      <td>2.120504</td>\n",
       "      <td>8.335341</td>\n",
       "      <td>0.0772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>hope</td>\n",
       "      <td>1.889781</td>\n",
       "      <td>6.617917</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>get better</td>\n",
       "      <td>1.821617</td>\n",
       "      <td>6.181849</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>okay</td>\n",
       "      <td>1.497448</td>\n",
       "      <td>4.470267</td>\n",
       "      <td>0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>care</td>\n",
       "      <td>1.443521</td>\n",
       "      <td>4.235581</td>\n",
       "      <td>0.4939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word(s)  coefficient   exp_coef  compound\n",
       "1066        help     3.816397  45.440198    0.4019\n",
       "827    feel like     2.697899  14.848507    0.3612\n",
       "2725     want to     2.660107  14.297823    0.0772\n",
       "2856        wish     2.628148  13.848098    0.4019\n",
       "904       friend     2.547431  12.774250    0.4939\n",
       "2724        want     2.120504   8.335341    0.0772\n",
       "1089        hope     1.889781   6.617917    0.4404\n",
       "940   get better     1.821617   6.181849    0.4404\n",
       "1746        okay     1.497448   4.470267    0.2263\n",
       "441         care     1.443521   4.235581    0.4939"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_best[ordered_best['compound'] > 0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Sentiment Analysis, I could determine that if the absolute value was of the compound score was high, then the word had a lot of weight in determining if the post title was in the r/depression subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Look at some Misclassifications\n",
    "\n",
    "---\n",
    "In order to look at some misclassifications, I will pickle load my test data from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "thoughts =  pd.read_csv('saved_data/token_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pickle.load(open('./assets/X_stem_test.pkl','rb'))\n",
    "y_test = pickle.load(open('./assets/y_stem_test.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 1132,  5661,  7368,  1806, 19737, 18053,  3661, 18942,  4653,\n",
       "             1001,\n",
       "            ...\n",
       "            18963,  8041, 13709,  7462,  3370, 16096,  9493, 12767,  4279,\n",
       "             2189],\n",
       "           dtype='int64', length=6600)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "preds_best = best.predict(X_test)\n",
    "\n",
    "# create a dataframe\n",
    "best_mis = pd.DataFrame([preds_best,y_test]).T\n",
    "\n",
    "# rename the columns\n",
    "best_mis.rename(columns={0:'preds',1:'y_test'},inplace=True)\n",
    "\n",
    "# set index to y_test index\n",
    "best_mis.set_index(y_test.index, inplace=True)\n",
    "\n",
    "# preview\n",
    "best_mis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will grab the indices where we misclassify a post and try to infer as to why they were misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mis=best_mis[best_mis['preds'] != best_mis['y_test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6216</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13696</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18484</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11908</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7418</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       preds  y_test\n",
       "3661       1       0\n",
       "8692       1       0\n",
       "6216       1       0\n",
       "15035      0       1\n",
       "13696      0       1\n",
       "...      ...     ...\n",
       "878        1       0\n",
       "18484      0       1\n",
       "11908      0       1\n",
       "7418       1       0\n",
       "1787       1       0\n",
       "\n",
       "[517 rows x 2 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_misclassifications = pd.concat([thoughts.iloc[best_mis.index]['title'],best_mis],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>preds</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>This is true</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>Everything is loud at night.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6216</th>\n",
       "      <td>Why?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>In the garden</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13696</th>\n",
       "      <td>just lost the eraser on top of my mechanical p...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>Christmas pumpkins</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18484</th>\n",
       "      <td>The Pressure is getting to me.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11908</th>\n",
       "      <td>Forget a person you considered everything</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7418</th>\n",
       "      <td>It must suck to suck.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>A new type of punishment</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  preds  y_test\n",
       "3661                                        This is true      1       0\n",
       "8692                        Everything is loud at night.      1       0\n",
       "6216                                                Why?      1       0\n",
       "15035                                      In the garden      0       1\n",
       "13696  just lost the eraser on top of my mechanical p...      0       1\n",
       "...                                                  ...    ...     ...\n",
       "878                                   Christmas pumpkins      1       0\n",
       "18484                     The Pressure is getting to me.      0       1\n",
       "11908          Forget a person you considered everything      0       1\n",
       "7418                               It must suck to suck.      1       0\n",
       "1787                            A new type of punishment      1       0\n",
       "\n",
       "[517 rows x 3 columns]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_misclassifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple reasons as to why I think these titles were misclassified:\n",
    "\n",
    "+ The Sentiment analysis might have thought that the title as a whole was more negative than positive and with the way most of words are in the \"depression\" subreddit, it might have classified it as such.\n",
    "+ The tokens of the words were classified in one subreddit but when looking at the original title, it could have avoided misclassification.\n",
    "+ I didn't pull enough data to be able to perfect my model\n",
    "+ The n-grams weren't perfected in figuring out some context of words. \n",
    "+ The inclusion of stop words may have added overfitting.\n",
    "\n",
    "There are many reasons as to why there was misclassification, but these are the ones I could think of.\n",
    "\n",
    "---\n",
    "Below are my misclassifications on my `forest` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5661</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7368</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19737</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       preds  y_test\n",
       "1132       0       0\n",
       "5661       0       0\n",
       "7368       0       0\n",
       "1806       1       0\n",
       "19737      1       1"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get predictions\n",
    "preds_forest = forest.predict(X_test)\n",
    "\n",
    "# create a dataframe\n",
    "forest_mis = pd.DataFrame([preds_forest,y_test]).T\n",
    "\n",
    "# rename the columns\n",
    "forest_mis.rename(columns={0:'preds',1:'y_test'},inplace=True)\n",
    "\n",
    "# set index to y_test index\n",
    "forest_mis.set_index(y_test.index, inplace=True)\n",
    "\n",
    "# preview\n",
    "forest_mis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_mis=forest_mis[forest_mis['preds'] != forest_mis['y_test']]\n",
    "forest_misclassifications = pd.concat([thoughts.iloc[forest_mis.index]['title'],forest_mis],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>preds</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6382</th>\n",
       "      <td>Fish out of water die by being itchy to death</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16917</th>\n",
       "      <td>Deppresion is like a war. you either win or yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5662</th>\n",
       "      <td>An escalator cannot break, it can only become ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9490</th>\n",
       "      <td>Talking</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10451</th>\n",
       "      <td>Being sexually assault as a kid and barely com...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  preds  y_test\n",
       "6382       Fish out of water die by being itchy to death      1       0\n",
       "16917  Deppresion is like a war. you either win or yo...      0       1\n",
       "5662   An escalator cannot break, it can only become ...      1       0\n",
       "9490                                             Talking      1       0\n",
       "10451  Being sexually assault as a kid and barely com...      0       1"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_misclassifications.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And my `another` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5661</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7368</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19737</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       preds  y_test\n",
       "1132       0       0\n",
       "5661       0       0\n",
       "7368       0       0\n",
       "1806       0       0\n",
       "19737      1       1"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get predictions\n",
    "preds_another = another.predict(X_test)\n",
    "\n",
    "# create a dataframe\n",
    "another_mis = pd.DataFrame([preds_another,y_test]).T\n",
    "\n",
    "# rename the columns\n",
    "another_mis.rename(columns={0:'preds',1:'y_test'},inplace=True)\n",
    "\n",
    "# set index to y_test index\n",
    "another_mis.set_index(y_test.index, inplace=True)\n",
    "\n",
    "# preview\n",
    "another_mis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_mis=another_mis[another_mis['preds'] != another_mis['y_test']]\n",
    "another_misclassifications = pd.concat([thoughts.iloc[another_mis.index]['title'],another_mis],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>preds</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16434</th>\n",
       "      <td>Pennsylvania area free counseling?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16039</th>\n",
       "      <td>Anyone here watching We Are Who We Are?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3976</th>\n",
       "      <td>That feeling that you get post-vomit feels bet...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3479</th>\n",
       "      <td>Happy wife happy life. Nothing rhymes with hus...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6216</th>\n",
       "      <td>Why?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  preds  y_test\n",
       "16434                 Pennsylvania area free counseling?      0       1\n",
       "16039            Anyone here watching We Are Who We Are?      0       1\n",
       "3976   That feeling that you get post-vomit feels bet...      1       0\n",
       "3479   Happy wife happy life. Nothing rhymes with hus...      1       0\n",
       "6216                                                Why?      1       0"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_misclassifications.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Scores\n",
    "\n",
    "---\n",
    "Just to check another metric, I decided to view the F1 scores. I wanted to see how accurate my models were and it seems to me that `another` model might be better than all of my models. Only reason why I am hesitant about these scores is because it ignores the true negatives.\n",
    "\n",
    "*Note: Both `best` and `another` are logistic regression models on the data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Averages of precision and sensitivity\n",
      "\n",
      "Best model: 0.9218915243994561\n",
      "Forest model: 0.9186787142645534\n",
      "Another model: 0.9330959798249518\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('Weighted Averages of precision and sensitivity')\n",
    "print()\n",
    "print('Best model:', metrics.f1_score(y_test, preds_best))\n",
    "print('Forest model:', metrics.f1_score(y_test, preds_forest))\n",
    "print('Another model:',metrics.f1_score(y_test, preds_another))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Conclusions\n",
    "\n",
    "The models I have produced aren't perfect at classifying if a post is from r/depression or r/Showerthoughts. If given more time to perfect as well as understand the content of the subreddits, I am sure that I can reduce the amount of false negatives. Since I was trying to figure out if a post was from depression or not, I want to be able to increase the specificity. The reason I would want this is to make it more applicable in real world to identify depression through social media posts. If I could, I would want to analyze posts from outlets such as Twitter and/or Facebook and try to get data on the person's mental state. But since that would be a very sensitive topic, having to classify if a post is from the depression subreddit or not is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
